{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习课上内容， 阅读相应论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答以下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  What is autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "autoencoder是一个先通过encoder把输入数据压缩成保留输入数据最大信息的低维数据，再通过decoder把latent space中的低维数据还原为和高维数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the differences between greedy search and beam search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "greedy search是每一步搜索只选取概率最大的一个结果进行搜索，只能保证每一步最优，不能保证最终结果最优\n",
    "\n",
    "beam search是第一步选取beam个概率最大结果，然后每个beam继续选取概率最大的一个结果，直到EOS，最终在beam个结果中选取概率最大的结果，保证局部最优\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the intuition of attention mechanism?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在普通的seq2seq模型中，encoder把所有输入编成一个context向量，即encoder最后一个隐藏层的状态，然后喂给decoder解码，但是对于一个长句子来说一个context向量可能没法保留整个句子的信息，并且当decoder解码第一个词的时候，很明第一个词与encoder的前几个词相关性最大，所以产生了将encoder所有隐藏层的状态赋予不同的权重作为decoder的一个输入，这就是注意力机制的直觉。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the disadvantage of word embeding introduced in previous lectures ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词向量没法解释一词多义现象，一个词只有一个词向量，bank有银行和河岸的意思，但是在word embedding词向量矩阵中只有一个向量表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What is the architecture of ELMo model. (A brief description is enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "双层双向LSTM，每一层的LSTM输出一个隐状态，进行加权平均，再乘以一个尺度缩放系数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compared to RNN,  what is the advantage of Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer对于输入可以并行处理，不像RNN每一时刻输入一个词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Why we use layer normalizaiton instead of batch normalization in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "层归一化是在一个sample里句子长度这个维度进行归一化，批归一化是在batch这个维度进行归一化，使用层归一化不受句子维度的影响。且由于Transformer是同时将词向量作为输入，所以层归一化是有道理的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Why we need position embedding in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer由于是将词向量同时输入进来处理的，所以需要原始句子里面词和词的位置关系，于是引入位置向量和词向量相加作为输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Briefly describe what is self-attention and what is multi-head attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自注意力机制有q,k,v三个向量，q和k点积运算再乘以一个缩放系数，然后进行softmax，得到的结果作为权重和v相乘得到新的词向量表示，为什么叫自注意力，这个‘自’体现在一句话里不同词语，通过迭代更新wq,wk,wv,矩阵，可以自己学得相似程度高的词之间的联系。而那个典型的seq2seq里面的encoder-decoder的注意力机制是根据decoder序列里面的词和encoder序列里面词的相似程度来判断得到权重。对比一下，seq2seq里面的decoder是注意到了encoder里面的输入词的权重信息，相当于‘外部’注意；而Transformer里面根据输入词向量，迭代更新wq,wk,wv，内部学的，不同词之间的权重信息，相当于‘内部’注意，即自注意力。\n",
    "\n",
    "多头自注意力机制就是做多次这样的自注意力机制运算。每次自注意力机制关注词之间不同位置上不同程度的关联，最后平均。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. What is the basic unit of GPT model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本单元是一个基于decoder的单向Transformer，即注意力只能和该预测词之前的词有关。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Briefly descibe how to use GPT in other NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类任务：transformer后面加一个linear层，softmax。\n",
    "\n",
    "命名实体识别：同上加linear层，命名实体识别其实也相当于一个多分类任务，需和CRF结合。\n",
    "\n",
    "相似度判断：两个tansformer融合加一个linear层。\n",
    "\n",
    "问答任务：多个transformer融合加一个线性层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is masked language model in BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于encoder的双向Transformer采用MLM掩盖部分词，进行无监督训练，采用MLM是为了使得一个词在深层双向训练过程中使用周围的词对mask掉的词进行更好地表征。\n",
    "\n",
    "BERT模型中作者对15%的word token进行mask，而被mask的词，有80%概率真正mask，10%概率替换为其它词，10%概率不变。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are the inputs of BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input = token embedding + segment embedding + positional embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Briely descibe how to use BERT in other NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类任务：取出第一个token[CLS]的最后一层隐状态，经过一个linear层分类\n",
    "\n",
    "QA任务：取出[sep]后所有token的最后一层隐状态，经过linear层\n",
    "\n",
    "sentence tag任务：取出所有token的最后一层隐状态，经过linear层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. What are the differences between these three models: GPT, BERT, GPT2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT:基于decoder的单向Transformer,mask self attention\n",
    "    \n",
    "GPT2:相对于GPT更多参数，更多数据\n",
    "\n",
    "BERT：基于encoder的双向Transformer，mask language model。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
